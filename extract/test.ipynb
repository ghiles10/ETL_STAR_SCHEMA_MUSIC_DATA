{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/AIRFLOW_MUSIC')\n",
    "from util.utils import spark_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song(cur, path) :\n",
    "\n",
    "    # spark_session = spark_session()     \n",
    "\n",
    "    # for root, dir, file in os.walk('/workspaces/AIRFLOW_MUSIC/data/song_data'):\n",
    "    #     for f in file: \n",
    "\n",
    "    df = spark_session.read.json(str(os.path.join(root, f)) ) \n",
    "    \n",
    "    # insert song record\n",
    "    df = spark_session.read.json(path)\n",
    "    song_data = tuple((df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).collect()[0]))\n",
    "    cur.execute(song_table_insert, song_data)\n",
    "\n",
    "\n",
    "    # insert artist record\n",
    "    df = spark_session.read.json(path)\n",
    "    song_data = tuple((df.select(['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']).collect()[0]))\n",
    "    cur.execute(artist_table_insert, song_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_file(cur, filepath):\n",
    "     \n",
    "    # open log file\n",
    "    df = spark_session.read.json(\"/workspaces/AIRFLOW_MUSIC/data/log_data/2018/11/2018-11-01-events.json\")\n",
    "\n",
    "    # filter by NextSong action\n",
    "    df = df.filter(df[\"page\"] == \"NextSong\")\n",
    "\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.to_datetime(df['ts'])\n",
    "\n",
    "    # insert time data records\n",
    "    time_data = [(tt.value, tt.hour, tt.day, tt.week, tt.month, tt.year, tt.weekday()) for tt in t]\n",
    "    column_labels = ('timestamp', 'hour', 'day', 'week', 'month', 'year', 'weekday')\n",
    "    time_df = pd.DataFrame(data=time_data, columns=column_labels)\n",
    "\n",
    "    for i, row in time_df.iterrows():\n",
    "        cur.execute(time_table_insert, list(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/13 19:09:11 WARN Utils: Your hostname, codespaces-a6bdc4 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/01/13 19:09:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/13 19:09:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def format_datetime(ts):\n",
    "\n",
    "    \"\"\"\n",
    "    Description: converts numeric timestamp to datetime format.\n",
    "    Returns:\n",
    "        timestamp with type datetime\n",
    "    \"\"\"\n",
    "\n",
    "    import datetime\n",
    "    return datetime.datetime.fromtimestamp(ts/1000.0)\n",
    "\n",
    "spark_session = spark_session()\n",
    "\n",
    "df = spark_session.read.json(\"/workspaces/AIRFLOW_MUSIC/data/log_data/2018/11/2018-11-01-events.json\")\n",
    "\n",
    "\n",
    "from pyspark.sql.types import DateType , TimestampType \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: format_datetime(int(x)), TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(df.ts))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: format_datetime(int(x)), DateType())\n",
    "df = df.withColumn(\"datetime\", get_datetime(df.ts))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select( 'start_time', 'datetime' , \n",
    "                        hour(\"datetime\").alias('hour'),\n",
    "                        dayofmonth(\"datetime\").alias('day'),\n",
    "                        weekofyear(\"datetime\").alias('week'),\n",
    "                        year(\"datetime\").alias('year'),\n",
    "                        month(\"datetime\").alias('month'),\n",
    "                        dayofweek(\"datetime\").alias('weekday') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
